{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Reference: https://www.kaggle.com/code/gusthema/asl-fingerspelling-recognition-w-tensorflow#Data-visualization-using-mediapipe-APIs","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nbase_input_path='/kaggle/input/asl-fingerspelling/'\nbase_output_path='/kaggle/working/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-19T11:20:47.478360Z","iopub.execute_input":"2023-08-19T11:20:47.479531Z","iopub.status.idle":"2023-08-19T11:20:47.532097Z","shell.execute_reply.started":"2023-08-19T11:20:47.479485Z","shell.execute_reply":"2023-08-19T11:20:47.531186Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:20:47.537332Z","iopub.execute_input":"2023-08-19T11:20:47.539672Z","iopub.status.idle":"2023-08-19T11:21:02.780098Z","shell.execute_reply.started":"2023-08-19T11:20:47.539618Z","shell.execute_reply":"2023-08-19T11:21:02.778898Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.1.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.5.26)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.7.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.23.5)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.8.0.74)\nRequirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.20.3)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.8.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\nInstalling collected packages: sounddevice, mediapipe\nSuccessfully installed mediapipe-0.10.3 sounddevice-0.4.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\" , category=UserWarning)\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport tensorflow as tf\nimport json\nimport mediapipe\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random\n\nfrom skimage.transform import resize\nfrom mediapipe.framework.formats import landmark_pb2\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tqdm.notebook import tqdm\nfrom matplotlib import animation, rc","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:21:02.782394Z","iopub.execute_input":"2023-08-19T11:21:02.783156Z","iopub.status.idle":"2023-08-19T11:21:11.323479Z","shell.execute_reply.started":"2023-08-19T11:21:02.783117Z","shell.execute_reply":"2023-08-19T11:21:11.322436Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(\"TensorFlow v\" + tf.__version__)\nprint(\"Mediapipe v\" + mediapipe.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:21:11.327328Z","iopub.execute_input":"2023-08-19T11:21:11.328667Z","iopub.status.idle":"2023-08-19T11:21:11.334491Z","shell.execute_reply.started":"2023-08-19T11:21:11.328628Z","shell.execute_reply":"2023-08-19T11:21:11.333470Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"TensorFlow v2.12.0\nMediapipe v0.10.3\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_df_path=f'{base_input_path}train.csv'\ndataset_df = pd.read_csv(dataset_df_path)\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))\ndataset_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:21:11.335867Z","iopub.execute_input":"2023-08-19T11:21:11.336941Z","iopub.status.idle":"2023-08-19T11:21:11.537239Z","shell.execute_reply.started":"2023-08-19T11:21:11.336904Z","shell.execute_reply":"2023-08-19T11:21:11.536285Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Full train dataset shape is (67208, 5)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                              path  file_id  sequence_id  participant_id  \\\n0  train_landmarks/5414471.parquet  5414471   1816796431             217   \n1  train_landmarks/5414471.parquet  5414471   1816825349             107   \n2  train_landmarks/5414471.parquet  5414471   1816909464               1   \n3  train_landmarks/5414471.parquet  5414471   1816967051              63   \n4  train_landmarks/5414471.parquet  5414471   1817123330              89   \n\n                      phrase  \n0               3 creekhouse  \n1            scales/kuhaylah  \n2        1383 william lanier  \n3          988 franklin lane  \n4  6920 northeast 661st road  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>file_id</th>\n      <th>sequence_id</th>\n      <th>participant_id</th>\n      <th>phrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816796431</td>\n      <td>217</td>\n      <td>3 creekhouse</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816825349</td>\n      <td>107</td>\n      <td>scales/kuhaylah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816909464</td>\n      <td>1</td>\n      <td>1383 william lanier</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816967051</td>\n      <td>63</td>\n      <td>988 franklin lane</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1817123330</td>\n      <td>89</td>\n      <td>6920 northeast 661st road</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# The numbers come from: https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\nLPOSE = [13, 15, 17, 19, 21]\nRPOSE = [14, 16, 18, 20, 22]\nPOSE = LPOSE + RPOSE","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:21:11.538855Z","iopub.execute_input":"2023-08-19T11:21:11.539190Z","iopub.status.idle":"2023-08-19T11:21:11.544924Z","shell.execute_reply.started":"2023-08-19T11:21:11.539158Z","shell.execute_reply":"2023-08-19T11:21:11.543604Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\nY = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\nZ = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\nFEATURE_COLUMNS = X + Y + Z\n# FEATURE_COLUMNS","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:21:11.547100Z","iopub.execute_input":"2023-08-19T11:21:11.547425Z","iopub.status.idle":"2023-08-19T11:21:11.556177Z","shell.execute_reply.started":"2023-08-19T11:21:11.547398Z","shell.execute_reply":"2023-08-19T11:21:11.555206Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"x_\" in col]\nY_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"y_\" in col]\nZ_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"z_\" in col]\n\nRHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\nLHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\nRPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\nLPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:21:11.557556Z","iopub.execute_input":"2023-08-19T11:21:11.558678Z","iopub.status.idle":"2023-08-19T11:21:11.567943Z","shell.execute_reply.started":"2023-08-19T11:21:11.558518Z","shell.execute_reply":"2023-08-19T11:21:11.567048Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#* problem: not sure why set to 128 here\n# Set length of frames to 128\nFRAME_LEN = 128\n\n# Create directory to store the new data\nif not os.path.isdir(\"preprocessed\"):\n    os.mkdir(\"preprocessed\")\nelse:\n    shutil.rmtree(\"preprocessed\")\n    os.mkdir(\"preprocessed\")\n\n# Loop through each file_id\nfor file_id in tqdm(dataset_df.file_id.unique()):\n    # Parquet file name\n    pq_file_path=f\"{base_input_path}train_landmarks/{file_id}.parquet\"\n    # Filter train.csv and fetch entries only for the relevant file_id\n    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n    # Fetch the parquet file\n    parquet_df = pq.read_table(pq_file_path,columns=['sequence_id'] + FEATURE_COLUMNS).to_pandas()\n    # File name for the updated data\n    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n    parquet_numpy = parquet_df.to_numpy()\n    # Initialize the pointer to write the output of \n    # each `for loop` below as a sequence into the file.\n    with tf.io.TFRecordWriter(tf_file) as file_writer:\n        # Loop through each sequence in file.\n        for seq_id, phrase in zip(file_df.sequence_id, file_df.phrase):\n            # Fetch sequence data\n            frames = parquet_numpy[parquet_df.index == seq_id]\n            \n            # Calculate the number of NaN values in each hand landmark\n            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n            no_nan = max(r_nonan, l_nonan)\n            \n            if 2*len(phrase)<no_nan:\n                features = {FEATURE_COLUMNS[i]: tf.train.Feature(\n                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(FEATURE_COLUMNS))}\n                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(phrase, 'utf-8')]))\n                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n                file_writer.write(record_bytes)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:21:11.569440Z","iopub.execute_input":"2023-08-19T11:21:11.570158Z","iopub.status.idle":"2023-08-19T11:32:25.179604Z","shell.execute_reply.started":"2023-08-19T11:21:11.570123Z","shell.execute_reply":"2023-08-19T11:32:25.178401Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/68 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b2635888c049abae9a003711392477"}},"metadata":{}}]},{"cell_type":"code","source":"tf_records = dataset_df.file_id.map(lambda x: f'{base_output_path}/preprocessed/{x}.tfrecord').unique()\nprint(f\"List of {len(tf_records)} TFRecord files.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:32:25.184000Z","iopub.execute_input":"2023-08-19T11:32:25.184749Z","iopub.status.idle":"2023-08-19T11:32:25.251960Z","shell.execute_reply.started":"2023-08-19T11:32:25.184708Z","shell.execute_reply":"2023-08-19T11:32:25.250772Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"List of 68 TFRecord files.\n","output_type":"stream"}]},{"cell_type":"code","source":"with open (f\"{base_input_path}character_to_prediction_index.json\", \"r\") as f:\n    char_to_num = json.load(f)\n\n# Add pad_token, start pointer and end pointer to the dict\npad_token = 'P'\nstart_token = '<'\nend_token = '>'\npad_token_idx = 59\nstart_token_idx = 60\nend_token_idx = 61\n\nchar_to_num[pad_token] = pad_token_idx\nchar_to_num[start_token] = start_token_idx\nchar_to_num[end_token] = end_token_idx\nnum_to_char = {j:i for i,j in char_to_num.items()}","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:50.546279Z","iopub.execute_input":"2023-08-19T11:37:50.546698Z","iopub.status.idle":"2023-08-19T11:37:50.554033Z","shell.execute_reply.started":"2023-08-19T11:37:50.546664Z","shell.execute_reply":"2023-08-19T11:37:50.553062Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Function to resize and add padding.\n# Reference: https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\ndef resize_pad(x):\n    if tf.shape(x)[0] < FRAME_LEN:\n        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n    else:\n        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n    return x\n\n# Detect the dominant hand from the number of NaN values.\n# Dominant hand will have less NaN values since it is in frame moving.\ndef pre_process(x):\n    rhand = tf.gather(x, RHAND_IDX, axis=1)\n    lhand = tf.gather(x, LHAND_IDX, axis=1)\n    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n    \n    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n    \n    rnans = tf.math.count_nonzero(rnan_idx)\n    lnans = tf.math.count_nonzero(lnan_idx)\n    \n    # For dominant hand\n    if rnans > lnans:\n        hand = lhand\n        pose = lpose\n        \n        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n        \n        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n    else:\n        hand = rhand\n        pose = rpose\n    \n    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n    \n    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n    hand = (hand - mean) / std\n\n    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n    \n    x = tf.concat([hand, pose], axis=1)\n    x = resize_pad(x)\n    \n    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:50.597151Z","iopub.execute_input":"2023-08-19T11:37:50.597457Z","iopub.status.idle":"2023-08-19T11:37:50.619109Z","shell.execute_reply.started":"2023-08-19T11:37:50.597432Z","shell.execute_reply":"2023-08-19T11:37:50.617901Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def decode_fn(record_bytes):\n    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n    features = tf.io.parse_single_example(record_bytes, schema)\n    phrase = features[\"phrase\"]\n    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n    # Transpose to maintain the original shape of landmarks data.\n    landmarks = tf.transpose(landmarks)\n    \n    return landmarks, phrase","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:50.621808Z","iopub.execute_input":"2023-08-19T11:37:50.622970Z","iopub.status.idle":"2023-08-19T11:37:50.630953Z","shell.execute_reply.started":"2023-08-19T11:37:50.622932Z","shell.execute_reply":"2023-08-19T11:37:50.630024Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"table = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(char_to_num.keys()),\n        values=list(char_to_num.values()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\ndef convert_fn(landmarks, phrase):\n    # Add start and end pointers to phrase.\n    phrase = start_token + phrase + end_token\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    # Vectorize and add padding.\n    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n                    constant_values = pad_token_idx)\n    # Apply pre_process function to the landmarks.\n    return pre_process(landmarks), phrase","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:50.632460Z","iopub.execute_input":"2023-08-19T11:37:50.633060Z","iopub.status.idle":"2023-08-19T11:37:54.754700Z","shell.execute_reply.started":"2023-08-19T11:37:50.633003Z","shell.execute_reply":"2023-08-19T11:37:54.753578Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\ntrain_len = int(0.8 * len(tf_records))\n\ntrain_ds = tf.data.TFRecordDataset(tf_records[:train_len]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\nvalid_ds = tf.data.TFRecordDataset(tf_records[train_len:]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:54.757231Z","iopub.execute_input":"2023-08-19T11:37:54.757645Z","iopub.status.idle":"2023-08-19T11:37:56.491097Z","shell.execute_reply.started":"2023-08-19T11:37:54.757609Z","shell.execute_reply":"2023-08-19T11:37:56.490009Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n\n\nclass LandmarkEmbedding(layers.Layer):\n    def __init__(self, num_hid=64, maxlen=100):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv1D(\n            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n        )\n        self.conv2 = tf.keras.layers.Conv1D(\n            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n        )\n        self.conv3 = tf.keras.layers.Conv1D(\n            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n        )\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return self.conv3(x)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:56.492869Z","iopub.execute_input":"2023-08-19T11:37:56.493271Z","iopub.status.idle":"2023-08-19T11:37:56.505790Z","shell.execute_reply.started":"2023-08-19T11:37:56.493220Z","shell.execute_reply":"2023-08-19T11:37:56.504422Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:56.507115Z","iopub.execute_input":"2023-08-19T11:37:56.508554Z","iopub.status.idle":"2023-08-19T11:37:56.518952Z","shell.execute_reply.started":"2023-08-19T11:37:56.508517Z","shell.execute_reply":"2023-08-19T11:37:56.517712Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Customized to add `training` variable\n# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target, training):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n        return ffn_out_norm\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:56.521462Z","iopub.execute_input":"2023-08-19T11:37:56.521769Z","iopub.status.idle":"2023-08-19T11:37:56.538258Z","shell.execute_reply.started":"2023-08-19T11:37:56.521743Z","shell.execute_reply":"2023-08-19T11:37:56.537510Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Customized to add edit_dist metric and training variable.\n# Reference:\n# https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\n# https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n\nclass Transformer(keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=60,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.acc_metric = keras.metrics.Mean(name=\"edit_dist\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target, training):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n        return y\n\n    def call(self, inputs, training):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source, training)\n        y = self.decode(x, target, training)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Computes the Levenshtein distance between sequences since the evaluation\n        # metric for this contest is the normalized total levenshtein distance.\n        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), \n                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n        edit_dist = tf.reduce_mean(edit_dist)\n        self.acc_metric.update_state(edit_dist)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n\n    def test_step(self, batch):        \n        source = batch[0]\n        target = batch[1]\n\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        \n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        # Computes the Levenshtein distance between sequences since the evaluation\n        # metric for this contest is the normalized total levenshtein distance.\n        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), \n                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n        edit_dist = tf.reduce_mean(edit_dist)\n        self.acc_metric.update_state(edit_dist)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source, training = False)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input, training = False)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = logits[:, -1][..., tf.newaxis]\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:56.541692Z","iopub.execute_input":"2023-08-19T11:37:56.542281Z","iopub.status.idle":"2023-08-19T11:37:56.570604Z","shell.execute_reply.started":"2023-08-19T11:37:56.542223Z","shell.execute_reply":"2023-08-19T11:37:56.569687Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class DisplayOutputs(keras.callbacks.Callback):\n    def __init__(\n        self, batch, idx_to_token, target_start_token_idx=60, target_end_token_idx=61\n    ):\n        \"\"\"Displays a batch of outputs after every 4 epoch\n\n        Args:\n            batch: A test batch\n            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n            target_start_token_idx: A start token index in the target vocabulary\n            target_end_token_idx: An end token index in the target vocabulary\n        \"\"\"\n        self.batch = batch\n        self.target_start_token_idx = target_start_token_idx\n        self.target_end_token_idx = target_end_token_idx\n        self.idx_to_char = idx_to_token\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 4 != 0:\n            return\n        source = self.batch[0]\n        target = self.batch[1].numpy()\n        bs = tf.shape(source)[0]\n        preds = self.model.generate(source, self.target_start_token_idx)\n        preds = preds.numpy()\n        for i in range(bs):\n            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n            prediction = \"\"\n            for idx in preds[i, :]:\n                prediction += self.idx_to_char[idx]\n                if idx == self.target_end_token_idx:\n                    break\n            print(f\"target:     {target_text.replace('-','')}\")\n            print(f\"prediction: {prediction}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:56.574984Z","iopub.execute_input":"2023-08-19T11:37:56.575363Z","iopub.status.idle":"2023-08-19T11:37:56.590524Z","shell.execute_reply.started":"2023-08-19T11:37:56.575330Z","shell.execute_reply":"2023-08-19T11:37:56.588776Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Transformer variables are customized from original keras tutorial to suit this dataset.\n# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\nbatch = next(iter(valid_ds))\n\n# The vocabulary to convert predicted indices into characters\nidx_to_char = list(char_to_num.keys())\ndisplay_cb = DisplayOutputs(\n    batch, idx_to_char, target_start_token_idx=char_to_num['<'], target_end_token_idx=char_to_num['>']\n)  # set the arguments as per vocabulary index for '<' and '>'\n\nmodel = Transformer(\n    num_hid=200,\n    num_head=4,\n    num_feed_forward=400,\n    source_maxlen = FRAME_LEN,\n    target_maxlen=64,\n    num_layers_enc=2,\n    num_layers_dec=1,\n    num_classes=62\n)\nloss_fn = tf.keras.losses.CategoricalCrossentropy(\n    from_logits=True, label_smoothing=0.1,\n)\n\n\noptimizer = keras.optimizers.Adam(0.0001)\nmodel.compile(optimizer=optimizer, loss=loss_fn)\n\nhistory = model.fit(train_ds, validation_data=valid_ds, callbacks=[display_cb], epochs=13)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:37:56.592088Z","iopub.execute_input":"2023-08-19T11:37:56.593760Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/13\n    632/Unknown - 111s 140ms/step - loss: 0.8754 - edit_dist: 1.0755target:     <2796 west golden willow drive>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <775 coreth strere rere>\n\ntarget:     <9734719887>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <+39-313-1999>\n\ntarget:     <4977236992>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <598-202-1636>\n\ntarget:     <reallyloud.co.uk/simaii>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <colleron corom>\n\ntarget:     <kkaicd1.pixnet.net>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <hartalalalan>\n\ntarget:     <8260 john r bowdoin>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <886 maran rerore ch re>\n\ntarget:     <56 paper birch drive>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <560-303-3838>\n\ntarget:     <gandchudaihardcor.html>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <300-505-5003>\n\ntarget:     <2708 west 77th>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <716-601-7878>\n\ntarget:     <https://www.keainfo.gr>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <https://www.com/>\n\ntarget:     <288 fuller lake>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <288-598-5113>\n\ntarget:     <mser/okiguide>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <eran rillle>\n\ntarget:     <220 north 47th avenue east>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <223 hanth hanane>\n\ntarget:     <www.sudinfo.be>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <www.ndrerere>\n\ntarget:     <69 grant point>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <369 charth co>\n\ntarget:     <fibrain.pl/chapter145/thanghr>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <901-211-111-1-1-11>\n\ntarget:     <+3515218895>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <+33-353-3598-5-58-8>\n\ntarget:     <viapierogobetti/brandilove>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <230 sininich chine>\n\ntarget:     <amir le>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <anen rine>\n\ntarget:     <automantenimientosa>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <maresenenesest>\n\ntarget:     <6499 nfd 5053>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <659-979-7773>\n\ntarget:     <7870 preston place>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <390-801-8189>\n\ntarget:     <6870 scabisuit lane>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <670 salan alalan>\n\ntarget:     <sunshine mayer>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <anthan hanerere>\n\ntarget:     <4082494707>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <405-773-5-30>\n\ntarget:     <arabradio.us/vana>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <astasalere coran>\n\ntarget:     <televisoresponse>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <5200 core cort>\n\ntarget:     <www.voices.com/bitesize>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <www.cor.com//1050009>\n\ntarget:     <qd.razavi.ac.ir/bunya>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <https://ww.comararareran>\n\ntarget:     <+246987508002294>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <+66-88-8-897-79-39>\n\ntarget:     <+2666480514>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <+22-080-808-08-9>\n\ntarget:     <2257645994>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <237-559-5955>\n\ntarget:     <35816 webwood>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <3880 coro ch>\n\ntarget:     <liana deleon>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <alan sanane>\n\ntarget:     </p21sistemas/sharps_carbines>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <http:///parestherererere>\n\ntarget:     <www.boehringeringelheim.com>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <www.coringhing.chom/>\n\ntarget:     <2901092582>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <290-917-1727>\n\ntarget:     <102 flagstad>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <139 st calan>\n\ntarget:     </verkstader/20180101>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <322 doron dert re>\n\ntarget:     <myanmarsocialonlinemedia.com>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <anane comerest>\n\ntarget:     <bunkerbranch.tumblr.com>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <4223 nanth rorore rour>\n\ntarget:     <+8286698863353>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <+88-424-4-23-23333>\n\ntarget:     <9464 east woods edge way>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <525 sananane rore>\n\ntarget:     <4632526021>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <466-333-3333>\n\ntarget:     <martin+blando/>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <alare cortere>\n\ntarget:     <beloshveyka.ru/105>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <belanantan ranere>\n\ntarget:     <bid.auctionbymayo.com/71063>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <ereste cortest corart>\n\ntarget:     <7384391127>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <339-539-3-31>\n\ntarget:     <www.twitchquotes.com/>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <www.stallis.com/>\n\ntarget:     <5016 east paquin street>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <560 stasth stest st st>\n\ntarget:     <2292979260>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <295-767-2666>\n\ntarget:     <9186054830>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <981-878-8-82>\n\ntarget:     <brandee tran>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <brene rerere>\n\ntarget:     <5061 williamsburg drive east>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <collin steraneste ran>\n\ntarget:     <certuspro.com/bloqueosjalapa>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <caresestestestestest>\n\ntarget:     <www.tudecora.com>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <www.comererestere>\n\ntarget:     <+279053128>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <+23-999-9-31-7-78-17>\n\ntarget:     <rachael salas>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <http:///wwarerane>\n\ntarget:     <9525366907>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <953-363-6000>\n\ntarget:     <963 crested owl>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <9933 rort coroll>\n\ntarget:     <2886 will teasley>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <2886 cllalal lal>\n\ntarget:     <josie savage>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <comanesterest st>\n\ntarget:     <4127407457>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <447-014-7747>\n\ntarget:     <www.tornadoshop.cz/cornelius>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\nprediction: <www.noronderth conde>\n\n632/632 [==============================] - 134s 177ms/step - loss: 0.8754 - edit_dist: 1.0755 - val_loss: 0.7957 - val_edit_dist: 1.0687\nEpoch 2/13\n632/632 [==============================] - 27s 43ms/step - loss: 0.7442 - edit_dist: 1.0683 - val_loss: 0.7160 - val_edit_dist: 1.0676\nEpoch 3/13\n632/632 [==============================] - 27s 43ms/step - loss: 0.6714 - edit_dist: 1.0665 - val_loss: 0.6596 - val_edit_dist: 1.0644\nEpoch 4/13\n631/632 [============================>.] - ETA: 0s - loss: 0.6254 - edit_dist: 1.0629","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training loss', 'val_loss'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TFLiteModel(tf.Module):\n    def __init__(self, model):\n        super(TFLiteModel, self).__init__()\n        self.target_start_token_idx = start_token_idx\n        self.target_end_token_idx = end_token_idx\n        # Load the feature generation and main models\n        self.model = model\n    \n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(FEATURE_COLUMNS)], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs, training=False):\n        # Preprocess Data\n        x = tf.cast(inputs, tf.float32)\n        x = x[None]\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(FEATURE_COLUMNS))), lambda: tf.identity(x))\n        x = x[0]\n        x = pre_process(x)\n        x = x[None]\n        x = self.model.generate(x, self.target_start_token_idx)\n        x = x[0]\n        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n        x = x[1:idx]\n        x = tf.one_hot(x, 59)\n        return {'outputs': x}\n    \ntflitemodel_base = TFLiteModel(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\nkeras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = keras_model_converter.convert()\nwith open(f'{base_output_path}model.tflite', 'wb') as f:\n    f.write(tflite_model)\n    \ninfargs = {\"selected_columns\" : FEATURE_COLUMNS}\n\nwith open('inference_args.json', \"w\") as json_file:\n    json.dump(infargs, json_file)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip submission.zip  './model.tflite' './inference_args.json'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(\"model.tflite\")\n\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\n\nwith open (f\"{base_input_path}character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\nfound_signatures = list(interpreter.get_signature_list().keys())\n\nif REQUIRED_SIGNATURE not in found_signatures:\n    raise KernelEvalException('Required input signature not found.')\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=batch[0][0])\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}